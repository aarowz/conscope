# ConScope

Real-time event ticket price monitoring system using Kafka to track concert prices across multiple sources and alert users when prices drop.

## Features

- âœ… **Real-Time Price Monitoring**: Track ticket prices from multiple sources (SeatGeek, Ticketmaster, Mock)
- âœ… **Price Change Detection**: Automatically detect price drops and increases in real-time
- âœ… **Price Drop Alerts**: Get notified via Discord webhooks when prices drop
- âœ… **Price History Tracking**: Store all price updates in PostgreSQL for historical analysis
- âœ… **Interactive Dashboard**: Visualize price trends, alerts, and statistics with Streamlit
- âœ… **Distributed Architecture**: Event-driven system built with Apache Kafka for scalability
- âœ… **Mock Producer**: Test the system without API keys using realistic mock data

## Tech Stack

- **Message Queue**: Apache Kafka + Zookeeper
- **Backend**: Python, Kafka Streams
- **Storage**: PostgreSQL
- **Dashboard**: Streamlit
- **Deployment**: Docker Compose

## Architecture

```
API Producers â†’ Kafka Topics â†’ Stream Processors â†’ Consumers
                                                   â†“
                                           PostgreSQL
                                           Alert Service
                                           Dashboard
```

## Getting Started

### Prerequisites

- Python 3.10+ (3.12+ not recommended due to kafka-python compatibility)
- Docker & Docker Compose
- API keys for ticket sources (optional - mock producer works without keys)

### Installation

```bash
# Clone repository
git clone https://github.com/aarowz/conscope.git
cd conscope

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Add your API keys to .env
```

### Running the System

#### Quick Start (Using Mock Producer)

For testing without API keys:

```bash
# 1. Start infrastructure
docker-compose up -d

# 2. Initialize database
python scripts/init_db.py

# 3. Create Kafka topics
python kafka_setup/create_topics.py

# 4. Start all components (in separate terminals)
# Terminal 1: Storage Consumer
python -m consumers.storage_consumer

# Terminal 2: Price Processor
python -m processors.price_processor

# Terminal 3: Alert Consumer
python -m consumers.alert_consumer

# Terminal 4: Mock Producer (generates test data)
python -m producers.mock_producer

# Terminal 5: Dashboard
streamlit run dashboard/app.py
```

Navigate to `http://localhost:8501` to view the dashboard.

#### Production Setup (With Real API Keys)

```bash
# 1. Set up environment variables
cp .env.example .env
# Edit .env and add your API keys

# 2. Start infrastructure
docker-compose up -d

# 3. Initialize database
python scripts/init_db.py

# 4. Create Kafka topics
python kafka_setup/create_topics.py

# 5. Start components (in separate terminals)
python -m producers.seatgeek_producer  # Requires SEATGEEK_API_KEY
python -m processors.price_processor
python -m consumers.storage_consumer
python -m consumers.alert_consumer
streamlit run dashboard/app.py
```

#### Automated Testing

Run the complete pipeline test:

```bash
# Test all components together for 30 seconds
python tests/test_pipeline_quick.py

# Or run full test (until Ctrl+C)
python tests/test_complete_pipeline.py
```

## Project Structure

```
conscope/
â”œâ”€â”€ producers/          # API pollers that publish to Kafka
â”‚   â”œâ”€â”€ base_producer.py      # Abstract base class for producers
â”‚   â”œâ”€â”€ mock_producer.py      # Mock producer for testing
â”‚   â”œâ”€â”€ seatgeek_producer.py  # SeatGeek API producer
â”‚   â””â”€â”€ config.py             # Producer configuration
â”œâ”€â”€ processors/         # Stream processing logic
â”‚   â””â”€â”€ price_processor.py    # Price change detection
â”œâ”€â”€ consumers/          # Kafka consumers
â”‚   â”œâ”€â”€ storage_consumer.py   # Stores events in PostgreSQL
â”‚   â””â”€â”€ alert_consumer.py     # Sends Discord notifications
â”œâ”€â”€ dashboard/          # Streamlit dashboard
â”‚   â””â”€â”€ app.py                # Main dashboard application
â”œâ”€â”€ models/            # Data models
â”‚   â””â”€â”€ price_event.py        # PriceEvent dataclass
â”œâ”€â”€ kafka_setup/       # Kafka configuration
â”‚   â”œâ”€â”€ config.py             # Topic configurations
â”‚   â””â”€â”€ create_topics.py      # Topic creation script
â”œâ”€â”€ scripts/           # Utility scripts
â”‚   â”œâ”€â”€ init_db.py            # Database schema initialization
â”‚   â””â”€â”€ run_pipeline.sh        # Pipeline runner script
â”œâ”€â”€ tests/              # Test scripts
â”‚   â”œâ”€â”€ test_complete_pipeline.py
â”‚   â”œâ”€â”€ test_mock_producer.py
â”‚   â”œâ”€â”€ test_storage_consumer.py
â”‚   â”œâ”€â”€ test_price_processor.py
â”‚   â””â”€â”€ test_pipeline_quick.py
â”œâ”€â”€ docs/              # Documentation
â”‚   â”œâ”€â”€ guides/               # User guides
â”‚   â””â”€â”€ archive/              # Historical docs
â”œâ”€â”€ docker-compose.yml  # Infrastructure orchestration
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md
```

## Data Schema

### PriceEvent

```python
{
    "event_id": "concert_123",
    "event_name": "Taylor Swift Eras",
    "venue": "MetLife Stadium",
    "date": "2025-05-15T19:00:00Z",
    "source": "seatgeek",
    "section": "Floor 2",
    "row": "15",
    "seat": "8",
    "price": 245.50,
    "timestamp": "2025-01-15T14:30:00Z",
    "url": "https://seatgeek.com/..."
}
```

## System Metrics

- **Processing Capacity**: 10k+ price updates per hour
- **Alert Latency**: <60 seconds from price drop to notification
- **Data Sources**: Mock (testing), SeatGeek (production-ready), Ticketmaster (planned)
- **Concurrent Events**: Monitor multiple events simultaneously
- **Dashboard**: Real-time visualization with auto-refresh

## Testing

The system includes comprehensive test scripts in the `tests/` directory:

- `tests/test_mock_producer.py` - Test mock producer in isolation
- `tests/test_storage_consumer.py` - Test storage consumer
- `tests/test_price_processor.py` - Test price change detection
- `tests/test_complete_pipeline.py` - End-to-end pipeline test
- `tests/test_pipeline_quick.py` - Quick 30-second test

See `docs/guides/TEST_PIPELINE.md` for detailed testing instructions.

## Roadmap

### âœ… Completed (MVP)

- [x] Kafka infrastructure setup (Zookeeper, Kafka, Kafka UI)
- [x] PostgreSQL database schema and initialization
- [x] Base producer architecture with error handling
- [x] Mock producer for testing without API keys
- [x] SeatGeek producer (ready for API key)
- [x] Storage consumer with connection pooling
- [x] Price change detection processor
- [x] Price drop alert system
- [x] Discord webhook notifications
- [x] Streamlit dashboard with interactive visualizations
- [x] End-to-end pipeline testing

### ðŸš€ Future Enhancements

- [ ] Ticketmaster producer integration
- [ ] Email notifications (in addition to Discord)
- [ ] Price prediction ML model
- [ ] Deduplication across sources
- [ ] Optimal buy time analysis
- [ ] User alert management (web UI)
- [ ] Production deployment guide
- [ ] Kubernetes/Docker Swarm orchestration

## API Sources

- [SeatGeek API](https://platform.seatgeek.com/)
- [Ticketmaster Discovery API](https://developer.ticketmaster.com/)
- StubHub (if available)
- Vivid Seats (if available)

## Documentation

- `README.md` - Main project documentation (you are here)
- `PROJECT_COMPLETE.md` - Project completion summary and demo guide
- `docs/guides/QUICKSTART.md` - Detailed setup guide
- `docs/guides/TEST_PIPELINE.md` - Testing instructions
- `docs/guides/MOCK_PRODUCER_GUIDE.md` - Using the mock producer
- `docs/guides/PRODUCER_SETUP.md` - Setting up real API producers

## Contributing

This is a personal learning project, but feedback and suggestions are welcome! Feel free to open an issue.

## Use Cases

While built for concert tickets, the architecture is extensible to any price tracking use case:

- Real estate (Zillow, Redfin)
- Stock prices
- Retail products
- Flight prices
- Cryptocurrency

## License

MIT License - see LICENSE file for details.

## Author

Aaron Zhou - [GitHub](https://github.com/aarowz) | [LinkedIn](https://linkedin.com/in/aaron-zihan-zhou)

---

**Built with Kafka for distributed, real-time event processing.**
